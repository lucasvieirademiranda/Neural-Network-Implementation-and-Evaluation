{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(reference_Y, predicted_Y):\n",
    "    \n",
    "    loss = 0\n",
    "    m = reference_Y.shape[0]\n",
    "    \n",
    "    for yt, yp in zip(reference_Y, predicted_Y):\n",
    "        \n",
    "        value = -1 * np.sum(yt * np.log(yp) + (1 - yt) * np.log(1 - yp))\n",
    "        \n",
    "        loss += value\n",
    "    \n",
    "    loss = (1 / m) * loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_nodes,\n",
    "                 first_hidden_nodes,\n",
    "                 second_hidden_nodes,\n",
    "                 output_nodes,\n",
    "                 learning_rate,\n",
    "                 hidden_activation_function,\n",
    "                 hidden_activation_derivative):\n",
    "        \n",
    "        self.input_nodes = input_nodes\n",
    "        self.first_hidden_nodes = first_hidden_nodes\n",
    "        self.second_hidden_nodes = second_hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.bwih = np.random.uniform(-0.1, 0.1, self.first_hidden_nodes).reshape(self.first_hidden_nodes, 1)\n",
    "        self.bwhh = np.random.uniform(-0.1, 0.1, self.second_hidden_nodes).reshape(self.second_hidden_nodes, 1)\n",
    "        self.bwho = np.random.uniform(-0.1, 0.1, self.output_nodes).reshape(self.output_nodes, 1)\n",
    "        \n",
    "        self.wih = np.random.uniform(-0.1, 0.1, self.first_hidden_nodes * self.input_nodes).reshape(self.first_hidden_nodes, self.input_nodes)\n",
    "        self.whh = np.random.uniform(-0.1, 0.1, self.second_hidden_nodes * self.first_hidden_nodes).reshape(self.second_hidden_nodes, self.first_hidden_nodes)\n",
    "        \n",
    "        hidden_nodes = self.first_hidden_nodes if (self.second_hidden_nodes == 0) else self.second_hidden_nodes\n",
    "        \n",
    "        self.who = np.random.uniform(-0.1, 0.1, self.output_nodes * hidden_nodes).reshape(self.output_nodes, hidden_nodes)\n",
    "        \n",
    "        self.hidden_activation_function = hidden_activation_function\n",
    "        self.hidden_activation_derivative = hidden_activation_derivative\n",
    "    \n",
    "    def _softmax(self, x):\n",
    "        \n",
    "        value = np.exp(x - np.max(x)) / np.sum(np.exp(x - np.max(x))) \n",
    "        return value\n",
    "    \n",
    "    def partial_fit(self, X, y):\n",
    "        \n",
    "        for inputs, targets in zip(X_normalized_training, y_normalized_training):\n",
    "\n",
    "            self._train(inputs, targets)\n",
    "    \n",
    "    def _train(self, inputs, targets):\n",
    "        \n",
    "        if (self.second_hidden_nodes == 0):\n",
    "            \n",
    "            self._one_hidden_layer_train(inputs, targets)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self._two_hidden_layer_train(inputs, targets)\n",
    "        \n",
    "    \n",
    "    def _one_hidden_layer_train(self, inputs, targets):\n",
    "        \n",
    "        inputs = np.array(inputs, ndmin = 2).T\n",
    "        targets = np.array(targets, ndmin = 2).T\n",
    "                \n",
    "\n",
    "        hidden_inputs = np.dot(self.wih, inputs) + (self.bwih * 1)\n",
    "        hidden_outputs = self.hidden_activation_function(hidden_inputs)\n",
    "        \n",
    "        final_inputs = np.dot(self.who, hidden_outputs) + (self.bwho * 1)\n",
    "        outputs = self._softmax(final_inputs)\n",
    "        \n",
    "        output_errors = targets - outputs\n",
    "        \n",
    "        self.bwho += self.learning_rate * output_errors\n",
    "        self.who += self.learning_rate * np.dot(output_errors, hidden_outputs.T)\n",
    "                \n",
    "        hidden_errors = np.dot(self.who.T, output_errors)\n",
    "        \n",
    "        self.bwih += self.learning_rate * (hidden_errors * self.hidden_activation_derivative(hidden_inputs))        \n",
    "        self.wih += self.learning_rate * np.dot((hidden_errors * self.hidden_activation_derivative(hidden_inputs)), inputs.T)\n",
    "        \n",
    "    \n",
    "    def _two_hidden_layer_train(self, inputs, targets):\n",
    "        \n",
    "        inputs = np.array(inputs, ndmin = 2).T\n",
    "        targets = np.array(targets, ndmin = 2).T\n",
    "        \n",
    "        first_hidden_inputs = np.dot(self.wih, inputs) + (self.bwih * 1)\n",
    "        first_hidden_outputs = self.hidden_activation_function(first_hidden_inputs)\n",
    "        \n",
    "        second_hidden_inputs = np.dot(self.whh, first_hidden_outputs) + (self.bwhh * 1)\n",
    "        second_hidden_outputs = self.hidden_activation_function(second_hidden_inputs)\n",
    "        \n",
    "        final_inputs = np.dot(self.who, second_hidden_outputs) + (self.bwho * 1)\n",
    "        outputs  = self._softmax(final_inputs)\n",
    "        \n",
    "        output_errors = targets - outputs\n",
    "        \n",
    "        self.bwho += self.learning_rate * output_errors\n",
    "        self.who += self.learning_rate * np.dot(output_errors, second_hidden_outputs.T)\n",
    "        \n",
    "        second_hidden_errors = np.dot(self.who.T, output_errors)\n",
    "        \n",
    "        self.bwhh += self.learning_rate * (second_hidden_errors * self.hidden_activation_derivative(second_hidden_inputs))\n",
    "        self.whh += self.learning_rate * np.dot((second_hidden_errors * self.hidden_activation_derivative(second_hidden_inputs)), first_hidden_outputs.T)\n",
    "        \n",
    "        first_hidden_errors = np.dot(self.whh.T, second_hidden_errors)\n",
    "        \n",
    "        self.bwih += self.learning_rate * (first_hidden_errors * self.hidden_activation_derivative(first_hidden_inputs))\n",
    "        self.wih += self.learning_rate * np.dot((first_hidden_errors * self.hidden_activation_derivative(first_hidden_inputs)), inputs.T)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        for inputs in X:\n",
    "            \n",
    "            output = self._query(inputs)\n",
    "            \n",
    "            output = np.argmax(output)\n",
    "            \n",
    "            outputs.append(output)\n",
    "            \n",
    "        outputs = np.array(outputs)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        for inputs in X:\n",
    "            \n",
    "            output = self._query(inputs)\n",
    "            \n",
    "            outputs.append(output)\n",
    "            \n",
    "        outputs = np.array(outputs)\n",
    "        \n",
    "        return outputs\n",
    "        \n",
    "    \n",
    "    def _query(self, inputs):\n",
    "        \n",
    "        result = np.array([])\n",
    "        \n",
    "        if (self.second_hidden_nodes == 0):\n",
    "            \n",
    "            result = self._one_hidden_layer_query(inputs)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            result = self._two_hidden_layer_query(inputs)\n",
    "    \n",
    "        return result\n",
    "    \n",
    "    def _one_hidden_layer_query(self, inputs):\n",
    "        \n",
    "        inputs = np.array(inputs, ndmin = 2).T\n",
    "        \n",
    "        hidden_inputs = np.dot(self.wih, inputs) + (self.bwih * 1)\n",
    "        hidden_outputs = self.hidden_activation_function(hidden_inputs)\n",
    "        \n",
    "        final_inputs = np.dot(self.who, hidden_outputs) + (self.bwho * 1)\n",
    "        final_outputs  = self._softmax(final_inputs)\n",
    "        \n",
    "        return final_outputs.ravel()\n",
    "    \n",
    "    def _two_hidden_layer_query(self, inputs):\n",
    "        \n",
    "        inputs = np.array(inputs, ndmin = 2).T\n",
    "        \n",
    "        first_hidden_inputs = np.dot(self.wih, inputs) + (self.bwih * 1)\n",
    "        first_hidden_outputs = self.hidden_activation_function(first_hidden_inputs)\n",
    "        \n",
    "        second_hidden_inputs = np.dot(self.whh, first_hidden_outputs) + (self.bwhh * 1)\n",
    "        second_hidden_outputs = self.hidden_activation_function(second_hidden_inputs)\n",
    "        \n",
    "        final_inputs = np.dot(self.who, second_hidden_outputs) + (self.bwho * 1)\n",
    "        final_outputs  = self._softmax(final_inputs)\n",
    "        \n",
    "        return final_outputs.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_csv('fashion-mnist_train.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = training_set.iloc[:, 1:].values, training_set.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Fold Cross Validation With Logistic Activation Function and One Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1 Accuracy: 0.87275 Time: 383.8347337245941s\n",
      "Fold: 2 Accuracy: 0.8628333333333333 Time: 305.77174735069275s\n",
      "Fold: 3 Accuracy: 0.8686666666666667 Time: 297.05134868621826s\n",
      "Fold: 4 Accuracy: 0.8606666666666667 Time: 303.55727887153625s\n",
      "Fold: 5 Accuracy: 0.8621666666666666 Time: 299.03385615348816s\n",
      "=====================================================================\n",
      "Fold Mean: 0.8654166666666668 Time: 1589.2534823417664S\n"
     ]
    }
   ],
   "source": [
    "k_folds = 5\n",
    "epochs = 60\n",
    "learning_rate = 0.001\n",
    "\n",
    "input_nodes = 784\n",
    "first_hidden_nodes = 15\n",
    "second_hidden_nodes = 0\n",
    "output_nodes = 10\n",
    "\n",
    "activation_function = lambda x : 1 / (1 + np.exp(-x))\n",
    "activation_derivative = lambda x : activation_function(x) * (1 - activation_function(x))\n",
    "\n",
    "k_fold = KFold(n_splits = k_folds, random_state = None, shuffle = False)\n",
    "\n",
    "accuracies = np.array([])\n",
    "\n",
    "current_fold = 0\n",
    "\n",
    "total_start = time.time()\n",
    "\n",
    "for train_index, validation_index in k_fold.split(X, y):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    current_fold += 1\n",
    "    \n",
    "    X_training, X_validation = X[train_index], X[validation_index]\n",
    "    y_training, y_validation = y[train_index], y[validation_index] \n",
    "    \n",
    "    # An grayscale image must have 255 as maximum value\n",
    "    maximum_value = 255 \n",
    "    \n",
    "    # Executes normalization between 0.01 and 0.99 to avoid lose of neurons\n",
    "    X_normalized_training = (X_training / maximum_value * 0.99) + 0.01\n",
    "    X_normalized_validation = (X_validation / maximum_value * 0.99) + 0.01\n",
    "    \n",
    "    # Executes one-hot-encoding on labels\n",
    "    y_normalized_training = pd.get_dummies(y_training).values\n",
    "    y_normalized_validation = pd.get_dummies(y_validation).values\n",
    "    \n",
    "    n = NeuralNetwork(input_nodes,\n",
    "                      first_hidden_nodes,\n",
    "                      second_hidden_nodes,\n",
    "                      output_nodes,\n",
    "                      learning_rate,\n",
    "                      activation_function,\n",
    "                      activation_derivative)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        n.partial_fit(X_normalized_training, y_normalized_training)\n",
    "        \n",
    "    y_validation_predicted = n.predict(X_normalized_validation)\n",
    "    \n",
    "    accuracy = accuracy_score(y_validation, y_validation_predicted)\n",
    "    \n",
    "    accuracies = np.append(accuracies, [accuracy])\n",
    "    \n",
    "    finish = time.time()\n",
    "    \n",
    "    print(\"Fold: \" + str(current_fold) + \" Accuracy: \" + str(accuracy) + \" Time: \" + str((finish - start)) + \"s\")\n",
    "\n",
    "total_finish = time.time()    \n",
    "\n",
    "print(\"=====================================================================\")\n",
    "print(\"Fold Mean: \" + str(accuracies.mean()) + \" Time: \" + str(total_finish - total_start) + \"S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Fold Cross Validation With Logistic Activation Function and Two Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1 Accuracy: 0.8511666666666666 Time: 402.18348836898804s\n",
      "Fold: 2 Accuracy: 0.8490833333333333 Time: 381.5738859176636s\n",
      "Fold: 3 Accuracy: 0.84975 Time: 388.8062937259674s\n",
      "Fold: 4 Accuracy: 0.8484166666666667 Time: 409.5522663593292s\n",
      "Fold: 5 Accuracy: 0.83925 Time: 403.2087891101837s\n",
      "=====================================================================\n",
      "Fold Mean: 0.8475333333333334 Time: 1985.3287348747253S\n"
     ]
    }
   ],
   "source": [
    "k_folds = 5\n",
    "epochs = 60\n",
    "learning_rate = 0.001\n",
    "\n",
    "input_nodes = 784\n",
    "first_hidden_nodes = 15\n",
    "second_hidden_nodes = 5\n",
    "output_nodes = 10\n",
    "\n",
    "activation_function = lambda x : 1 / (1 + np.exp(-x))\n",
    "activation_derivative = lambda x : activation_function(x) * (1 - activation_function(x))\n",
    "\n",
    "k_fold = KFold(n_splits = k_folds, random_state = None, shuffle = False)\n",
    "\n",
    "accuracies = np.array([])\n",
    "\n",
    "current_fold = 0\n",
    "\n",
    "total_start = time.time()\n",
    "\n",
    "for train_index, validation_index in k_fold.split(X, y):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    current_fold += 1\n",
    "    \n",
    "    X_training, X_validation = X[train_index], X[validation_index]\n",
    "    y_training, y_validation = y[train_index], y[validation_index] \n",
    "    \n",
    "    # An grayscale image must have 255 as maximum value\n",
    "    maximum_value = 255 \n",
    "    \n",
    "    # Executes normalization between 0.01 and 0.99 to avoid lose of neurons\n",
    "    X_normalized_training = (X_training / maximum_value * 0.99) + 0.01\n",
    "    X_normalized_validation = (X_validation / maximum_value * 0.99) + 0.01\n",
    "    \n",
    "    # Executes one-hot-encoding on labels\n",
    "    y_normalized_training = pd.get_dummies(y_training).values\n",
    "    y_normalized_validation = pd.get_dummies(y_validation).values\n",
    "    \n",
    "    n = NeuralNetwork(input_nodes,\n",
    "                      first_hidden_nodes,\n",
    "                      second_hidden_nodes,\n",
    "                      output_nodes,\n",
    "                      learning_rate,\n",
    "                      activation_function,\n",
    "                      activation_derivative)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        n.partial_fit(X_normalized_training, y_normalized_training)\n",
    "        \n",
    "    y_validation_predicted = n.predict(X_normalized_validation)\n",
    "    \n",
    "    accuracy = accuracy_score(y_validation, y_validation_predicted)\n",
    "    \n",
    "    accuracies = np.append(accuracies, [accuracy])\n",
    "    \n",
    "    finish = time.time()\n",
    "    \n",
    "    print(\"Fold: \" + str(current_fold) + \" Accuracy: \" + str(accuracy) + \" Time: \" + str((finish - start)) + \"s\")\n",
    "\n",
    "total_finish = time.time()    \n",
    "\n",
    "print(\"=====================================================================\")\n",
    "print(\"Fold Mean: \" + str(accuracies.mean()) + \" Time: \" + str(total_finish - total_start) + \"S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Fold Cross Validation With ReLU Activation Function and One Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1 Accuracy: 0.6004166666666667 Time: 268.31643986701965s\n",
      "Fold: 2 Accuracy: 0.5605 Time: 274.70882296562195s\n",
      "Fold: 3 Accuracy: 0.5483333333333333 Time: 271.36923360824585s\n",
      "Fold: 4 Accuracy: 0.6384166666666666 Time: 268.75539541244507s\n",
      "Fold: 5 Accuracy: 0.583 Time: 267.8313226699829s\n",
      "=====================================================================\n",
      "Fold Mean: 0.5861333333333333 Time: 1350.9842262268066S\n"
     ]
    }
   ],
   "source": [
    "k_folds = 5\n",
    "epochs = 60\n",
    "learning_rate = 0.001\n",
    "\n",
    "input_nodes = 784\n",
    "first_hidden_nodes = 15\n",
    "second_hidden_nodes = 0\n",
    "output_nodes = 10\n",
    "\n",
    "activation_function = lambda x : np.maximum(x, 0) \n",
    "activation_derivative = lambda x : 1 * (x > 0)\n",
    "\n",
    "k_fold = KFold(n_splits = k_folds, random_state = None, shuffle = False)\n",
    "\n",
    "accuracies = np.array([])\n",
    "\n",
    "current_fold = 0\n",
    "\n",
    "total_start = time.time()\n",
    "\n",
    "for train_index, validation_index in k_fold.split(X, y):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    current_fold += 1\n",
    "    \n",
    "    X_training, X_validation = X[train_index], X[validation_index]\n",
    "    y_training, y_validation = y[train_index], y[validation_index] \n",
    "    \n",
    "    # An grayscale image must have 255 as maximum value\n",
    "    maximum_value = 255 \n",
    "    \n",
    "    # Executes normalization between 0.01 and 0.99 to avoid lose of neurons\n",
    "    X_normalized_training = (X_training / maximum_value * 0.99) + 0.01\n",
    "    X_normalized_validation = (X_validation / maximum_value * 0.99) + 0.01\n",
    "    \n",
    "    # Executes one-hot-encoding on labels\n",
    "    y_normalized_training = pd.get_dummies(y_training).values\n",
    "    y_normalized_validation = pd.get_dummies(y_validation).values\n",
    "    \n",
    "    n = NeuralNetwork(input_nodes,\n",
    "                      first_hidden_nodes,\n",
    "                      second_hidden_nodes,\n",
    "                      output_nodes,\n",
    "                      learning_rate,\n",
    "                      activation_function,\n",
    "                      activation_derivative)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        n.partial_fit(X_normalized_training, y_normalized_training)\n",
    "        \n",
    "    y_validation_predicted = n.predict(X_normalized_validation)\n",
    "    \n",
    "    accuracy = accuracy_score(y_validation, y_validation_predicted)\n",
    "    \n",
    "    accuracies = np.append(accuracies, [accuracy])\n",
    "    \n",
    "    finish = time.time()\n",
    "    \n",
    "    print(\"Fold: \" + str(current_fold) + \" Accuracy: \" + str(accuracy) + \" Time: \" + str((finish - start)) + \"s\")\n",
    "\n",
    "total_finish = time.time()    \n",
    "\n",
    "print(\"=====================================================================\")\n",
    "print(\"Fold Mean: \" + str(accuracies.mean()) + \" Time: \" + str(total_finish - total_start) + \"S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Fold Cross Validation With ReLU Activation Function and Two Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\unicamp\\lib\\site-packages\\ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in maximum\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\ProgramData\\Anaconda3\\envs\\unicamp\\lib\\site-packages\\numpy\\core\\fromnumeric.py:83: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\unicamp\\lib\\site-packages\\ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in greater\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1 Accuracy: 0.10125 Time: 401.4645824432373s\n",
      "Fold: 2 Accuracy: 0.10141666666666667 Time: 390.5203847885132s\n",
      "Fold: 3 Accuracy: 0.0985 Time: 405.6593062877655s\n",
      "Fold: 4 Accuracy: 0.09891666666666667 Time: 396.02067041397095s\n",
      "Fold: 5 Accuracy: 0.09991666666666667 Time: 385.85765314102173s\n",
      "=====================================================================\n",
      "Fold Mean: 0.1 Time: 1979.5276100635529S\n"
     ]
    }
   ],
   "source": [
    "k_folds = 5\n",
    "epochs = 60\n",
    "learning_rate = 0.001\n",
    "\n",
    "input_nodes = 784\n",
    "first_hidden_nodes = 15\n",
    "second_hidden_nodes = 5\n",
    "output_nodes = 10\n",
    "\n",
    "activation_function = lambda x : np.maximum(x, 0) \n",
    "activation_derivative = lambda x : 1 * (x > 0)\n",
    "\n",
    "k_fold = KFold(n_splits = k_folds, random_state = None, shuffle = False)\n",
    "\n",
    "accuracies = np.array([])\n",
    "\n",
    "current_fold = 0\n",
    "\n",
    "total_start = time.time()\n",
    "\n",
    "for train_index, validation_index in k_fold.split(X, y):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    current_fold += 1\n",
    "    \n",
    "    X_training, X_validation = X[train_index], X[validation_index]\n",
    "    y_training, y_validation = y[train_index], y[validation_index] \n",
    "    \n",
    "    # An grayscale image must have 255 as maximum value\n",
    "    maximum_value = 255 \n",
    "    \n",
    "    # Executes normalization between 0.01 and 0.99 to avoid lose of neurons\n",
    "    X_normalized_training = (X_training / maximum_value * 0.99) + 0.01\n",
    "    X_normalized_validation = (X_validation / maximum_value * 0.99) + 0.01\n",
    "    \n",
    "    # Executes one-hot-encoding on labels\n",
    "    y_normalized_training = pd.get_dummies(y_training).values\n",
    "    y_normalized_validation = pd.get_dummies(y_validation).values\n",
    "    \n",
    "    n = NeuralNetwork(input_nodes,\n",
    "                      first_hidden_nodes,\n",
    "                      second_hidden_nodes,\n",
    "                      output_nodes,\n",
    "                      learning_rate,\n",
    "                      activation_function,\n",
    "                      activation_derivative)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        n.partial_fit(X_normalized_training, y_normalized_training)\n",
    "        \n",
    "    y_validation_predicted = n.predict(X_normalized_validation)\n",
    "    \n",
    "    accuracy = accuracy_score(y_validation, y_validation_predicted)\n",
    "    \n",
    "    accuracies = np.append(accuracies, [accuracy])\n",
    "    \n",
    "    finish = time.time()\n",
    "    \n",
    "    print(\"Fold: \" + str(current_fold) + \" Accuracy: \" + str(accuracy) + \" Time: \" + str((finish - start)) + \"s\")\n",
    "\n",
    "total_finish = time.time()    \n",
    "\n",
    "print(\"=====================================================================\")\n",
    "print(\"Fold Mean: \" + str(accuracies.mean()) + \" Time: \" + str(total_finish - total_start) + \"S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Fold Cross Validation With TanH Activation Function and One Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1 Accuracy: 0.8648333333333333 Time: 250.76187443733215s\n",
      "Fold: 2 Accuracy: 0.8618333333333333 Time: 250.45608687400818s\n",
      "Fold: 3 Accuracy: 0.8613333333333333 Time: 258.0417287349701s\n",
      "Fold: 4 Accuracy: 0.8645833333333334 Time: 249.24977087974548s\n",
      "Fold: 5 Accuracy: 0.8571666666666666 Time: 252.72898936271667s\n",
      "=====================================================================\n",
      "Fold Mean: 0.86195 Time: 1261.241458415985S\n"
     ]
    }
   ],
   "source": [
    "k_folds = 5\n",
    "epochs = 60\n",
    "learning_rate = 0.001\n",
    "\n",
    "input_nodes = 784\n",
    "first_hidden_nodes = 15\n",
    "second_hidden_nodes = 0\n",
    "output_nodes = 10\n",
    "\n",
    "activation_function = lambda x : np.tanh(x)\n",
    "activation_derivative = lambda x : 1.0 - np.tanh(x) ** 2\n",
    "\n",
    "k_fold = KFold(n_splits = k_folds, random_state = None, shuffle = False)\n",
    "\n",
    "accuracies = np.array([])\n",
    "\n",
    "current_fold = 0\n",
    "\n",
    "total_start = time.time()\n",
    "\n",
    "for train_index, validation_index in k_fold.split(X, y):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    current_fold += 1\n",
    "    \n",
    "    X_training, X_validation = X[train_index], X[validation_index]\n",
    "    y_training, y_validation = y[train_index], y[validation_index] \n",
    "    \n",
    "    # An grayscale image must have 255 as maximum value\n",
    "    maximum_value = 255 \n",
    "    \n",
    "    # Executes normalization between 0.01 and 0.99 to avoid lose of neurons\n",
    "    X_normalized_training = (X_training / maximum_value * 0.99) + 0.01\n",
    "    X_normalized_validation = (X_validation / maximum_value * 0.99) + 0.01\n",
    "    \n",
    "    # Executes one-hot-encoding on labels\n",
    "    y_normalized_training = pd.get_dummies(y_training).values\n",
    "    y_normalized_validation = pd.get_dummies(y_validation).values\n",
    "    \n",
    "    n = NeuralNetwork(input_nodes,\n",
    "                      first_hidden_nodes,\n",
    "                      second_hidden_nodes,\n",
    "                      output_nodes,\n",
    "                      learning_rate,\n",
    "                      activation_function,\n",
    "                      activation_derivative)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        n.partial_fit(X_normalized_training, y_normalized_training)\n",
    "        \n",
    "    y_validation_predicted = n.predict(X_normalized_validation)\n",
    "    \n",
    "    accuracy = accuracy_score(y_validation, y_validation_predicted)\n",
    "    \n",
    "    accuracies = np.append(accuracies, [accuracy])\n",
    "    \n",
    "    finish = time.time()\n",
    "    \n",
    "    print(\"Fold: \" + str(current_fold) + \" Accuracy: \" + str(accuracy) + \" Time: \" + str((finish - start)) + \"s\")\n",
    "\n",
    "total_finish = time.time()    \n",
    "\n",
    "print(\"=====================================================================\")\n",
    "print(\"Fold Mean: \" + str(accuracies.mean()) + \" Time: \" + str(total_finish - total_start) + \"S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Fold Cross Validation With TanH Activation Function and Two Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1 Accuracy: 0.86175 Time: 316.42951703071594s\n",
      "Fold: 2 Accuracy: 0.85675 Time: 321.8556385040283s\n",
      "Fold: 3 Accuracy: 0.8565 Time: 308.57216143608093s\n",
      "Fold: 4 Accuracy: 0.856 Time: 292.59344124794006s\n",
      "Fold: 5 Accuracy: 0.8525833333333334 Time: 293.26931071281433s\n",
      "=====================================================================\n",
      "Fold Mean: 0.8567166666666667 Time: 1532.7240707874298S\n"
     ]
    }
   ],
   "source": [
    "k_folds = 5\n",
    "epochs = 60\n",
    "learning_rate = 0.001\n",
    "\n",
    "input_nodes = 784\n",
    "first_hidden_nodes = 15\n",
    "second_hidden_nodes = 5\n",
    "output_nodes = 10\n",
    "\n",
    "activation_function = lambda x : np.tanh(x)\n",
    "activation_derivative = lambda x : 1.0 - np.tanh(x) ** 2\n",
    "\n",
    "k_fold = KFold(n_splits = k_folds, random_state = None, shuffle = False)\n",
    "\n",
    "accuracies = np.array([])\n",
    "\n",
    "current_fold = 0\n",
    "\n",
    "total_start = time.time()\n",
    "\n",
    "for train_index, validation_index in k_fold.split(X, y):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    current_fold += 1\n",
    "    \n",
    "    X_training, X_validation = X[train_index], X[validation_index]\n",
    "    y_training, y_validation = y[train_index], y[validation_index] \n",
    "    \n",
    "    # An grayscale image must have 255 as maximum value\n",
    "    maximum_value = 255 \n",
    "    \n",
    "    # Executes normalization between 0.01 and 0.99 to avoid lose of neurons\n",
    "    X_normalized_training = (X_training / maximum_value * 0.99) + 0.01\n",
    "    X_normalized_validation = (X_validation / maximum_value * 0.99) + 0.01\n",
    "    \n",
    "    # Executes one-hot-encoding on labels\n",
    "    y_normalized_training = pd.get_dummies(y_training).values\n",
    "    y_normalized_validation = pd.get_dummies(y_validation).values\n",
    "    \n",
    "    n = NeuralNetwork(input_nodes,\n",
    "                      first_hidden_nodes,\n",
    "                      second_hidden_nodes,\n",
    "                      output_nodes,\n",
    "                      learning_rate,\n",
    "                      activation_function,\n",
    "                      activation_derivative)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        n.partial_fit(X_normalized_training, y_normalized_training)\n",
    "        \n",
    "    y_validation_predicted = n.predict(X_normalized_validation)\n",
    "    \n",
    "    accuracy = accuracy_score(y_validation, y_validation_predicted)\n",
    "    \n",
    "    accuracies = np.append(accuracies, [accuracy])\n",
    "    \n",
    "    finish = time.time()\n",
    "    \n",
    "    print(\"Fold: \" + str(current_fold) + \" Accuracy: \" + str(accuracy) + \" Time: \" + str((finish - start)) + \"s\")\n",
    "\n",
    "total_finish = time.time()    \n",
    "\n",
    "print(\"=====================================================================\")\n",
    "print(\"Fold Mean: \" + str(accuracies.mean()) + \" Time: \" + str(total_finish - total_start) + \"S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
